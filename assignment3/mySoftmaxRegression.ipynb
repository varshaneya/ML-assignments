{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(J,theta,alpha,options):\n",
    "    for i in range(options['maxiter']):\n",
    "        _,grad = J(theta)\n",
    "        theta = theta - alpha * grad\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, alpha, options={'maxiter': 400, 'disp': True}):\n",
    "    \"\"\"\n",
    "    Train a softmax model with the given parameters on the given data.\n",
    "    Returns optimal theta, a vector containing the trained parameters\n",
    "    for the model.\n",
    "    input_size: the size of an input vector x^(i)\n",
    "    n_classes: the number of classes\n",
    "    lambda_: weight decay parameter\n",
    "    input_data: an N by M matrix containing the input data, such that\n",
    "                input_data[:, c] is the c-th input\n",
    "    labels: M by 1 matrix containing the class labels for the\n",
    "            corresponding inputs. labels[c] is the class label for\n",
    "            the c-th input\n",
    "    options (optional): options\n",
    "      options['maxiter']: number of iterations to train for\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    # Find out the optimal theta\n",
    "    opt_theta = gradientDescent(J,theta,alpha,options=options)\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = loadmat('./Data/mnistTrainImages.mat')\n",
    "X = X['trainData'].T\n",
    "y = loadmat('./Data/mnistTrainLabels.mat')\n",
    "y = y['trainLabels']\n",
    "X_test = loadmat('./Data/mnistTestImages.mat')\n",
    "X_test = X_test['testData'].T\n",
    "y_test = loadmat('./Data/mnistTestLabels.mat')\n",
    "y_test = y_test['testLabels']\n",
    "print X.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 1e-4 # Weight decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for learning rate 0.010000 accuracy is 0.870900\n",
      "for learning rate 0.100000 accuracy is 0.908400\n",
      "for learning rate 1.000000 accuracy is 0.922200\n",
      "for learning rate 10.000000 accuracy is 0.912900\n"
     ]
    }
   ],
   "source": [
    "options = {'maxiter': 1000, 'disp': True}\n",
    "alphas = [0.01,0.1,1,10] #learning rates\n",
    "for alpha in alphas:\n",
    "    model = softmax_train(input_size, n_classes, lambda_, X, y, alpha,options)\n",
    "    pred = softmax_predict(model, X_test)\n",
    "    print 'for learning rate %f accuracy is %f' % (alpha,accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of iterations | Learning rate | Accuracy (%)|\n",
    "|:--------------------:|:-------------:|:--------:|\n",
    "|100|0.01|78.75|\n",
    "|100|0.1|87.06|\n",
    "|100|1|90.85|\n",
    "|100|10|90.55|\n",
    "|500|0.01|84.97|\n",
    "|500|0.1|89.97|\n",
    "|500|1|91.94|\n",
    "|500|10|82.32|\n",
    "|1000|0.01|87.09|\n",
    "|1000|0.1|90.84|\n",
    "|1000|1|92.22|\n",
    "|1000|10|91.29|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Some observations </h3> </center>\n",
    "1. As the number of iterations increase the accuracy increases meaning that the algorithm is learning the actual theta.\n",
    "2. For learning rate equal to 1, there is maximum accuracy for a given number of iteration.\n",
    "3. The learning rate is a hyper-parameter and needs to be tuned according to the dataset. For this dataset the optimal value for alpha is 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
