{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-class logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise constants\n",
    "\n",
    "# error-rate is set\n",
    "error = 20\n",
    "\n",
    "#learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# add machine epsilon while finding log to avoid log(0)\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "# size of subset of training data to used. If it is none then whole of training set is used\n",
    "subsetSize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateLogits(data,theta):\n",
    "    return 1/(1+np.exp(-1*np.matmul(np.transpose(data),theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X_train,y_train,alpha,error):\n",
    "    # initialize theta to a random value\n",
    "    thetaNew = np.random.rand(featurevecSize,1)\n",
    "    thetaOld = np.inf\n",
    "    numIterations = 0\n",
    "    #print np.linalg.norm(thetaNew - thetaOld,1),error\n",
    "    while np.linalg.norm(thetaNew - thetaOld,2) > error or np.isnan(np.linalg.norm(thetaNew - thetaOld,2)):\n",
    "        thetaOld = thetaNew\n",
    "        thetaNew = thetaOld - (alpha * np.matmul(X_train,(calculateLogits(X_train,thetaOld)-y_train)))\n",
    "        numIterations+=1\n",
    "    return (thetaNew,numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading and preprocessing of data\n",
    "X_train = loadmat('mnistTrainImages.mat')\n",
    "y_train = loadmat('mnistTrainLabels.mat')\n",
    "    \n",
    "if subsetSize is not None and subsetSize < X_train['trainData'].shape[0]:\n",
    "    trainIndices = np.random.randint(0,X_train['trainData'].shape[0],subsetSize)\n",
    "else:\n",
    "    trainIndices = range(X_train['trainData'].shape[0])\n",
    "\n",
    "#convert to numpy array\n",
    "X_train = np.array(X_train['trainData'][trainIndices,:])\n",
    "y_train = np.array(y_train['trainLabels'][trainIndices,:])\n",
    "\n",
    "numClasses = len(np.unique(y_train))\n",
    "\n",
    "# prepend with a column of ones to account for bias and take transpose \n",
    "X_train = np.transpose(np.insert(X_train,0,1,1))\n",
    "\n",
    "trainSize = X_train.shape[1]\n",
    "featurevecSize = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load test data and test labels\n",
    "X_test = loadmat('mnistTestImages.mat')\n",
    "y_test = loadmat('mnistTestLabels.mat')\n",
    "\n",
    "X_test = np.array(X_test['testData'])\n",
    "y_test = np.array(y_test['testLabels'])\n",
    "\n",
    "testSize = X_test.shape[0]\n",
    "\n",
    "X_test = np.insert(X_test,0,1,1)\n",
    "\n",
    "prediction = np.zeros((testSize,numClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varshaneya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For error rate of 20 with learning rate of 0.01\n",
      "Accuracy in percentage 85.32\n"
     ]
    }
   ],
   "source": [
    "# train model using gradient descent method\n",
    "for label in range(numClasses):\n",
    "    modifiedLabelsTrain = copy.deepcopy(y_train)\n",
    "    modifiedLabelsTrain[np.where(y_train == label)[0]] = 1\n",
    "    modifiedLabelsTrain[np.where(y_train != label)[0]] = 0\n",
    "    (theta,_) = gradientDescent(X_train,modifiedLabelsTrain,alpha,error)\n",
    "    prediction[:,label] = calculateLogits(np.transpose(X_test),theta).ravel()\n",
    "\n",
    "finalPrediction = prediction.argmax(axis=1)\n",
    "\n",
    "print 'For error rate of',error,'with learning rate of',alpha\n",
    "print 'Accuracy in percentage',accuracy_score(finalPrediction,y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Alpha stands for learning rate and error tolerance is the distance (as measured by 2-norm) between successive theta values. These are called hyper-parameters because they are not estimated in the process of regression, but are fixed by the user before the start of the algorithm. As of now there is no closed form formula for calculating them but are set by trial and error.\n",
    "\n",
    "The following are accuracies and iterations to converge for various values of hyper-parameters alpha and error tolerance for threshold probability 0.5:\n",
    "\n",
    "|Error rate | Learning rate | Accuracy (%) |\n",
    "|:---------:|:-------------:|:--------:|\n",
    "| 10 | 0.01 | 80.87 |\n",
    "| 20 | 0.01 | 85.32 |\n",
    "| 5 | 0.001 | 83.96 |\n",
    "| 10 | 0.001 | 81.79 |\n",
    "| 20 | 0.001 | 80.91 |\n",
    "| 5 | 0.0001 | 62.2 |\n",
    "| 10 | 0.0001 | 63.46 |\n",
    "| 20 | 0.0001 | 63.73 |\n",
    "\n",
    "1. Gradient descent has got a very slow convergence often oscillates a lot for large datasets. Hence other variants of gradient descent like batch gradient descent and stochastic gradient descent.\n",
    "\n",
    "2. Threshold probability is not used here. The *argmax* function in numpy gives the index of the largest logit calculated for a particular test example. This is assigned as the predicted class which is compared to the original labels to calculate accuracy.\n",
    "\n",
    "3. It is clearly observed that for a lower learning, rate convergence of logits is quite faster. This is because alpha scales down the gradient value thus preventing a lot of oscillations. So it is ideal to choose a smaller learning rate.\n",
    "\n",
    "4. There is also a steady dip in accuracy along with decreasing learning rate. Hence there is a trade-off between speed and accuracy. That is for a lower learning rate the rate of convergence is fast but accuracy is low and vice versa. This is a common situation that one encounters in various other branches of computer science as well.\n",
    "\n",
    "5. With an increase in error tolerance the accuracy decreases slightly but the convergence happens faster and vice versa.\n",
    "\n",
    "6. From the above table ideal values for the parameters are alpha equals 0.01 and error tolerance equals 20 for this dataset. The hyper-parameters vary across datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
