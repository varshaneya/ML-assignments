{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To code a logistic regression for classifying MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise constants\n",
    "\n",
    "# error-rate is set\n",
    "error = 20\n",
    "\n",
    "#learning rate\n",
    "alpha = 0.001\n",
    "\n",
    "# add machine epsilon while finding log to avoid log(0)\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "# size of subset of training data to used. If it is none then whole of training set is used\n",
    "subsetSize = None\n",
    "\n",
    "# the threshold probability\n",
    "p0 = 0.7\n",
    "\n",
    "# set the number of epochs for gradient descent\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n"
     ]
    }
   ],
   "source": [
    "#loading and preprocessing of data\n",
    "X_train = loadmat('mnistTrainImages.mat')\n",
    "y_train = loadmat('mnistTrainLabels.mat')\n",
    "    \n",
    "if subsetSize is not None and subsetSize < X_train['trainData'].shape[0]:\n",
    "    trainIndices = np.random.randint(0,X_train['trainData'].shape[0],subsetSize)\n",
    "else:\n",
    "    trainIndices = range(X_train['trainData'].shape[0])\n",
    "\n",
    "#convert to numpy array\n",
    "X_train = np.array(X_train['trainData'][trainIndices,:])\n",
    "y_train = np.array(y_train['trainLabels'][trainIndices,:])\n",
    "\n",
    "# prepend with a column of ones to account for bias and take transpose \n",
    "X_train = np.transpose(np.insert(X_train,0,1,1))\n",
    "\n",
    "trainSize = X_train.shape[1]\n",
    "featurevecSize = X_train.shape[0]\n",
    "print featurevecSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateLogits(data,theta):\n",
    "    return 1/(1+np.exp(-1*np.matmul(np.transpose(data),theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(alpha,error):\n",
    "    # initialize theta to a random value\n",
    "    thetaNew = np.random.rand(featurevecSize,1)\n",
    "    thetaOld = np.inf\n",
    "    numIterations = 0\n",
    "    #print np.linalg.norm(thetaNew - thetaOld,1),error\n",
    "    while np.linalg.norm(thetaNew - thetaOld,2) > error or np.isnan(np.linalg.norm(thetaNew - thetaOld,2)):\n",
    "        #print np.linalg.norm(thetaNew - thetaOld,2)\n",
    "        thetaOld = thetaNew\n",
    "        thetaNew = thetaOld - (alpha * np.matmul(X_train,(calculateLogits(X_train,thetaOld)-y_train)))\n",
    "        numIterations+=1\n",
    "    return (thetaNew,numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateLogitsIndividual(xi,theta):\n",
    "    return 1/(1+np.exp(-1*np.matmul(np.transpose(theta),xi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochasticGradientDescent(alpha,epochs):\n",
    "    # initialize theta to a random value\n",
    "    thetaNew = np.random.rand(featurevecSize,1)\n",
    "    thetaOld = np.inf\n",
    "    #print np.linalg.norm(thetaNew - thetaOld,1),error\n",
    "    \n",
    "    #print trainData.shape,trainLabel.shape\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        data = np.hstack((np.transpose(X_train),y_train))\n",
    "        np.random.shuffle(data)\n",
    "        trainData = np.transpose(data[:,0:X_train.shape[1]])\n",
    "        trainLabel = data[:,X_train.shape[1]:X_train.shape[1]+1]\n",
    "        for i in range(trainSize) :\n",
    "            thetaNew = thetaNew - (alpha * np.matmul(X_train[:,i],X_train[:,i]*(calculateLogitsIndividual(X_train[:,i],thetaNew)-y_train[i])))\n",
    "    return thetaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load test data and test labels\n",
    "X_test = loadmat('mnistTestImages.mat')\n",
    "y_test = loadmat('mnistTestLabels.mat')\n",
    "\n",
    "X_test = np.array(X_test['testData'])\n",
    "y_test = np.array(y_test['testLabels'])\n",
    "\n",
    "X_test = np.insert(X_test,0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varshaneya/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For error tolerance of 20 with learning rate of 0.001 gradient descent converged in 7 iterations\n",
      "Accuracy in percentage 97.21\n"
     ]
    }
   ],
   "source": [
    "# train the model using gradient descent and then test for accuracy\n",
    "\n",
    "(theta,numIterations) = gradientDescent(alpha,error)\n",
    "\n",
    "prediction = calculateLogits(np.transpose(X_test),theta)\n",
    "\n",
    "positiveClass = np.where(prediction>=p0)[0]\n",
    "negativeClass = np.where(prediction<p0)[0]\n",
    "\n",
    "accuracy = float(len(np.where(y_test[positiveClass] == 1)[0]) + len(np.where(y_test[negativeClass] != 1)[0]))*100/y_test.shape[0]\n",
    "\n",
    "print 'For error tolerance of',error,'with learning rate of',alpha,'gradient descent converged in',numIterations,'iterations'\n",
    "print 'Accuracy in percentage',accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Alpha stands for learning rate and error tolerance is the distance (as measured by 2-norm) between successive theta values. These are called hyper-parameters because they are not estimated in the process of regression, but are fixed by the user before the start of the algorithm. As of now there is no closed form formula for calculating them but are set by trial and error. \n",
    "\n",
    "The following are accuracies and iterations to converge for various values of hyper-parameters alpha and error tolerance for threshold probability 0.5:\n",
    "\n",
    "|Error tolerance | Learning rate | Accuracy (%) | Iterations to Converge |\n",
    "|:---------:|:-------------:|:------------:|:----------------------:|\n",
    "| 15 | 0.1 | 99.05 | 1096 |\n",
    "| 20 | 0.1 | 99.12 | 387 |\n",
    "| 5 | 0.01 | 99.04 | 132 |\n",
    "| 10 | 0.01 | 98.75 | 58 |\n",
    "| 15 | 0.01 | 98.38 | 38 |\n",
    "| 20 | 0.01 | 98.16 | 27 |\n",
    "| 5 | 0.001 | 97.32 | 10 |\n",
    "| 10 | 0.001 | 97.35 | 9 |\n",
    "| 15 | 0.001 | 97.37 | 8 |\n",
    "| 20 | 0.001 | 97.21 | 7 |\n",
    "\n",
    "The following are accuracies and iterations to converge for various values of hyper-parameters alpha and error tolerance for threshold probability 0.2:\n",
    "\n",
    "|Error tolerance | Learning rate | Accuracy (%) | Iterations to Converge |\n",
    "|:---------:|:-------------:|:------------:|:----------------------:|\n",
    "| 15 | 0.1 | 99.06 | 1180 |\n",
    "| 20 | 0.1 | 99.12 | 387 |\n",
    "| 5 | 0.01 | 99.04 | 132 |\n",
    "| 10 | 0.01 | 98.75 | 58 |\n",
    "| 15 | 0.01 | 98.38 | 38 |\n",
    "| 20 | 0.01 | 98.16 | 27 |\n",
    "| 5 | 0.001 | 97.33 | 10 |\n",
    "| 10 | 0.001 | 97.34 | 9 |\n",
    "| 15 | 0.001 | 97.35 | 8 |\n",
    "| 20 | 0.001 | 97.24 | 7 |\n",
    "\n",
    "The following are accuracies and iterations to converge for various values of hyper-parameters alpha and error tolerance for threshold probability 0.8:\n",
    "\n",
    "|Error tolerance | Learning rate | Accuracy (%) | Iterations to Converge |\n",
    "|:---------:|:-------------:|:------------:|:----------------------:|\n",
    "| 15 | 0.1 | 99.0 | 1084 |\n",
    "| 20 | 0.1 | 99.12 | 387 |\n",
    "| 5 | 0.01 | 99.04 | 132 |\n",
    "| 10 | 0.01 | 98.75 | 58 |\n",
    "| 15 | 0.01 | 98.38 | 38 |\n",
    "| 20 | 0.01 | 98.17 | 27 |\n",
    "| 5 | 0.001 | 97.33 | 10 |\n",
    "| 10 | 0.001 | 97.32 | 9 |\n",
    "| 15 | 0.001 | 97.34 | 8 |\n",
    "| 20 | 0.001 | 97.24 | 7 |\n",
    "\n",
    "1. It is clearly observed that for a lower learning, rate convergence of logits is quite faster. This is because alpha scales down the gradient value thus preventing a lot of oscillations. So it is ideal to choose a smaller learning rate.\n",
    "\n",
    "2. There is also a steady dip in accuracy along with decreasing learning rate. Hence there is a trade-off between speed and accuracy. That is for a lower learning rate the rate of convergence is fast but accuracy is low and vice versa. This is a common situation that one encounters in various other branches of computer science as well.\n",
    "\n",
    "3. With an increase in error tolerance the accuracy decreases slightly but the convergence happens faster and vice versa.\n",
    "\n",
    "4. From the above table ideal values for the parameters are alpha equals 0.1 and error tolerance equals 20 for this dataset. The hyper-parameters vary across datasets.\n",
    "\n",
    "5. Comparing across the tables we seen that the accuracies across various hyper-parameters **do not change much**. So we can conclude that the probabilities calculated by the sigmoid function are **well-separated** in the sense that they are well *below 0.2* and *above 0.8* for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a learning rate of 0.001 with number of epochs to be 1 accuracy in percentage 90.2\n"
     ]
    }
   ],
   "source": [
    "#train using stochastic gradient descent method\n",
    "theta = stochasticGradientDescent(alpha,epoch)\n",
    "\n",
    "prediction = calculateLogits(np.transpose(X_test),theta)\n",
    "\n",
    "positiveClass = np.where(prediction>=p0)[0]\n",
    "negativeClass = np.where(prediction<p0)[0]\n",
    "\n",
    "accuracy = float(len(np.where(y_test[positiveClass] == 1)[0]) + len(np.where(y_test[negativeClass] != 1)[0]))*100/y_test.shape[0]\n",
    "\n",
    "print 'For a learning rate of',alpha,'with number of epochs to be',epoch,'accuracy in percentage',accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
