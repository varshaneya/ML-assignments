{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataloader():\n",
    "    boston = datasets.load_boston()\n",
    "    data = boston.data\n",
    "    labels = boston.target\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data,labels,test_size=0.2,random_state=3)\n",
    "    print X_train.shape\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradJ(theta,train,labels,numSamples):\n",
    "    return (np.matmul((np.matmul(theta.T,train)-labels),train.T).T)/numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(train,labels,alpha, options):\n",
    "    theta = np.random.randn(options['n_attrib'],1) #theta transpose\\n\",\n",
    "    while True:\n",
    "        thetaOld = theta\n",
    "        theta = theta - alpha * gradJ(theta,train,labels,options['num_samples'])\n",
    "        if np.linalg.norm(theta-thetaOld,2)<=options['tolerance']:\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "New X_train dim  (14, 404)\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-6,1e-7,1e-8,1e-9]\n",
    "tolerances = [1e-4,1e-5,1e-6,1e-7]\n",
    "trainData,testData,y_train,y_test = dataloader()\n",
    "X_train = np.transpose(np.insert(trainData,0,1,1))\n",
    "X_test = np.transpose(np.insert(testData,0,1,1))\n",
    "options = {'n_attrib':X_train.shape[0],'num_samples':X_train.shape[1],'tolerance':1e-7}\n",
    "print 'New X_train dim ',X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for tolerance 0.000100 with learning rate 0.000000 is 2056.258632\n",
      "Mean squared error for tolerance 0.000010 with learning rate 0.000000 is 1485.763928\n",
      "Mean squared error for tolerance 0.000001 with learning rate 0.000000 is 246.373733\n",
      "Mean squared error for tolerance 0.000000 with learning rate 0.000000 is 55.183478\n",
      "Mean squared error for tolerance 0.000100 with learning rate 0.000000 is 29392.222593\n",
      "Mean squared error for tolerance 0.000010 with learning rate 0.000000 is 3708.573371\n",
      "Mean squared error for tolerance 0.000001 with learning rate 0.000000 is 925.131850\n",
      "Mean squared error for tolerance 0.000000 with learning rate 0.000000 is 99.512109\n"
     ]
    }
   ],
   "source": [
    "# Trying to find the optimum value for learning rate and tolerance\n",
    "for alpha in alphas:\n",
    "    for tolerance in tolerances:\n",
    "        options['tolerance'] = tolerance\n",
    "        theta = gradientDescent(X_train,y_train,alpha,options)\n",
    "        prediction = np.matmul(np.transpose(theta),X_test).reshape(y_test.shape)\n",
    "        print 'Mean squared error for tolerance %f with learning rate %f is %f' \\\n",
    "        %(options['tolerance'],alpha,mean_squared_error(prediction,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE obtained using gradient descent:\n",
    "\n",
    "|Learning Rate|Tolerance|Mean Squared Error|\n",
    "|:-----------:|:-------:|:----------------:|\n",
    "|1e-6|1e-4|160.68|\n",
    "|1e-6|1e-5|51.89|\n",
    "|1e-6|1e-6|21.73|\n",
    "|1e-6|1e-7|19.07|\n",
    "|1e-7|1e-4|424.91|\n",
    "|1e-7|1e-5|115.79|\n",
    "|1e-7|1e-6|53.81|\n",
    "|1e-7|1e-7|21.49|\n",
    "|1e-8|1e-4|2056.26|\n",
    "|1e-8|1e-5|1485.76|\n",
    "|1e-8|1e-6|246.37|\n",
    "|1e-8|1e-7|55.48|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using gradient descent, for tolerance 0.000000 with learning rate 0.000001 is 19.070616\n",
      "Time taken is 746.630659 seconds\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e-6\n",
    "options['tolerance'] = 1e-7\n",
    "timeTaken = -time.time()\n",
    "theta = gradientDescent(X_train,y_train,alpha,options)\n",
    "timeTaken += time.time()\n",
    "prediction = np.matmul(np.transpose(theta),X_test).reshape(y_test.shape)\n",
    "print 'MSE using gradient descent, for tolerance %f with learning rate %f is %f' \\\n",
    "        %(options['tolerance'],alpha,mean_squared_error(prediction,y_test))\n",
    "print 'Time taken is %f seconds'%(timeTaken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using native linear regression method in sklearn 16.9682486675\n",
      "Time taken is 0.002954 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#X_train,X_test,y_train,y_test = dataloader()\n",
    "timeTaken = -time.time()\n",
    "model = LinearRegression()\n",
    "model.fit(trainData,y_train)\n",
    "timeTaken += time.time()\n",
    "predicted = model.predict(testData)\n",
    "print 'MSE using native linear regression method in sklearn',mean_squared_error(predicted,y_test)\n",
    "print 'Time taken is %f seconds'%(timeTaken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using scipy.optimize 19.7982317626\n",
      "Time taken is 0.643147 seconds\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "theta = np.random.randn(options['n_attrib'],1)\n",
    "J = lambda theta : mean_squared_error(np.matmul(theta.T,X_train).ravel(),y_train)\n",
    "timeTaken = -time.time()\n",
    "results = scipy.optimize.minimize(J,theta, method='L-BFGS-B', jac=False, options={'maxiter': 500, 'disp': True})\n",
    "timeTaken += time.time()\n",
    "optimumTheta = results['x']\n",
    "prediction = np.matmul(np.transpose(optimumTheta),X_test).reshape(y_test.shape)\n",
    "print 'MSE using scipy.optimize', mean_squared_error(prediction,y_test)\n",
    "print 'Time taken is %f seconds'%(timeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. It is quite clear from the table that learning rate of 1e-6 with tolerance 1e-7 is optimal as it gives the least MSE for this dataset.\n",
    "2. The MSE using gradient descent is 19.07 but took 746.63\n",
    "3. The MSE using native implementation of linear regression from sklearn is 16.97 but took 0.003 seconds.\n",
    "4. The MSE using minimize function from scipy.optimize using the method L-BFGS-B is 19.72 but took 0.64 seconds.\n",
    "5. So it is more efficient and accurate to use the native implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
